{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example AI Platform Training Job Submission\n",
    "---\n",
    "\n",
    "Purpose of this notebook is to test job submission to AI Platform outside of the context of an end-to-end TFX pipeline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET = 'ml-sandbox-tagging-tfx-experiments'\n",
    "PROJECT = 'ml-sandbox-101'\n",
    "REGION = 'europe-west1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import pandas as pd\n",
    "\n",
    "os.environ[\"PROJECT\"] = PROJECT\n",
    "os.environ[\"BUCKET\"] = BUCKET\n",
    "os.environ[\"REGION\"] = REGION\n",
    "os.environ[\"TFVERSION\"] = \"2.3\"\n",
    "os.environ[\"PYTHONVERSION\"] = \"3.7\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash \n",
    "\n",
    "mkdir -p trainer\n",
    "touch trainer/__init__.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing trainer/model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile  trainer/model.py\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_transform as tft\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras import callbacks, layers\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import Precision, Recall\n",
    "\n",
    "from google.cloud import storage\n",
    "import pickle\n",
    "\n",
    "EMBEDDING_LOCATION = 'gs://ml-sandbox-101-tagging/data/processed/training_data/glove_data/glove_embedding_index.pkl'\n",
    "MAX_STRING_LENGTH = 277\n",
    "\n",
    "\n",
    "def create_tag_lookup_table(tag_file):\n",
    "    table = tf.lookup.StaticVocabularyTable(\n",
    "        tf.lookup.TextFileInitializer(\n",
    "            tag_file,\n",
    "            key_dtype=tf.string, key_index=tf.lookup.TextFileIndex.WHOLE_LINE,\n",
    "            value_dtype=tf.int64, value_index=tf.lookup.TextFileIndex.LINE_NUMBER,\n",
    "            delimiter=None),\n",
    "        num_oov_buckets=1)\n",
    "    return table\n",
    "\n",
    "\n",
    "def label_transform(x, y, num_tags, table):\n",
    "    \"\"\"Use the number of classes to convert the sparse tag indicies to dense\"\"\"\n",
    "    # Need to add one for out-of-vocabulary tags in eval dataset\n",
    "    return (x, tf.cast(tf.sparse.to_indicator(table.lookup(y), vocab_size=num_tags + 1), tf.int32))\n",
    "\n",
    "#def _gzip_reader_fn(filenames):\n",
    "#    \"\"\"Small utility returning a record reader that can read gzip'ed fies\"\"\"\n",
    "#    return tf.data.TFRecordDataset(filenames) #, compression_type=\"GZIP\")\n",
    "\n",
    "\n",
    "def _input_fn(file_pattern, tf_transform_output, num_tags, table, batch_size=64, shuffle=True, epochs=None):\n",
    "    \"\"\"Generates features and label for tuning/training.\n",
    "    Args:\n",
    "        file_pattern: input tfrecord file pattern.\n",
    "        tf_transform_output: A TFTransformOutput.\n",
    "        batch_size: representing the number of consecutive elements of\n",
    "          returned dataset to combine in a single batch\n",
    "    Returns:\n",
    "        A dataset that contains (features, indices) tuple where features\n",
    "        is a dictionary of Tensors, and indices is a single Tensor of\n",
    "        label indices.\n",
    "    \"\"\"\n",
    "    transformed_feature_spec = (\n",
    "        tf_transform_output.transformed_feature_spec().copy()\n",
    "    )\n",
    "\n",
    "    dataset = tf.data.experimental.make_batched_features_dataset(\n",
    "        file_pattern=file_pattern,\n",
    "        batch_size=batch_size,\n",
    "        features=transformed_feature_spec,\n",
    "        reader=tf.data.TFRecordDataset, #_gzip_reader_fn,\n",
    "        shuffle=shuffle,\n",
    "        label_key='series_ep_tags',\n",
    "        num_epochs=epochs\n",
    "    )\n",
    "    return dataset.map(lambda x, y: label_transform(x, y, num_tags, table))\n",
    "\n",
    "class AutoTaggingModel:\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_file,\n",
    "        embedding_dim,\n",
    "        train_embedding,\n",
    "        output_size,\n",
    "        vocab_size,\n",
    "        vocab_df,\n",
    "        max_string_length,\n",
    "    ):\n",
    "        self.__embedding_file = embedding_file\n",
    "        self.__embedding_dim = embedding_dim\n",
    "        self.__vocab_size = vocab_size\n",
    "        self.__vocab_df = vocab_df\n",
    "        self.__train_embedding = train_embedding\n",
    "        self.__output_size = output_size\n",
    "        self.__max_string_length = max_string_length\n",
    "        \n",
    "        self.__initialize_embedding_matrix()\n",
    "    \n",
    "    def __initialize_embedding_matrix(self):\n",
    "        storage_client = storage.Client()\n",
    "        \n",
    "        # Better way to do this with os.path?\n",
    "        split_path = self.__embedding_file.split('/')\n",
    "        bucket_name = split_path[2]\n",
    "        blob_name = ('/').join(split_path[3:])\n",
    "        \n",
    "        bucket = storage_client.bucket(bucket_name)\n",
    "        blob = bucket.blob(blob_name)\n",
    "        \n",
    "        pickle_in = blob.download_as_string()\n",
    "        file = pickle.loads(pickle_in)\n",
    "        \n",
    "        self.embedding_matrix = np.zeros((self.__vocab_size, \n",
    "                                     self.__embedding_dim))\n",
    "        \n",
    "        for i, word in enumerate(self.__vocab_df.values):\n",
    "            embedding_vector = file.get(word[0])\n",
    "            if embedding_vector is not None:\n",
    "                self.embedding_matrix[i] = embedding_vector\n",
    "        \n",
    "    def embedding_layer(self):\n",
    "        return layers.Embedding(\n",
    "            input_dim=self.__vocab_size,\n",
    "            output_dim=self.__embedding_dim,\n",
    "            weights=[self.embedding_matrix],\n",
    "            input_length=self.__max_string_length,\n",
    "            trainable=self.__train_embedding,\n",
    "        )\n",
    "\n",
    "    def n_grams_channel(self, inputs, n_words_filter: int):\n",
    "        channel = layers.Conv2D(256, kernel_size=(n_words_filter, self.__embedding_dim), activation=\"relu\")(inputs)\n",
    "        channel_mp = layers.MaxPool2D(pool_size=(channel.shape[1], 1))(channel)\n",
    "        channel_final = layers.Flatten()(channel_mp)\n",
    "        return channel_final\n",
    "    \n",
    "    def define_model(self):\n",
    "        inputs = layers.Input(shape=(self.__max_string_length,), name='features')\n",
    "        embedding = self.embedding_layer()(inputs) \n",
    "        channel_inputs = layers.Reshape(target_shape=(self.__max_string_length, self.__embedding_dim, 1))(embedding)\n",
    "        channel1_final = self.n_grams_channel(channel_inputs, 3)\n",
    "        channel2_final = self.n_grams_channel(channel_inputs, 4)\n",
    "        channel3_final = self.n_grams_channel(channel_inputs, 5)\n",
    "        channels_final = layers.Concatenate()(\n",
    "            [channel1_final, channel2_final, channel3_final]\n",
    "        )\n",
    "        channels_final = layers.Dropout(rate=0.4)(channels_final)\n",
    "        channels_final = layers.Dense(2000, \"relu\")(channels_final)\n",
    "        predictions = layers.Dense(self.__output_size, \"sigmoid\")(channels_final)\n",
    "        model = Model(inputs=inputs, outputs=predictions)\n",
    "\n",
    "        return model\n",
    "\n",
    "    def get_model(self):\n",
    "        strategy = tf.distribute.MirroredStrategy()\n",
    "        with strategy.scope():\n",
    "            model = self.define_model()\n",
    "\n",
    "            metrics = [tf.keras.metrics.Precision(), tf.keras.metrics.Recall()]\n",
    "            model.compile(\n",
    "                optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005),\n",
    "                loss=BinaryCrossentropy(),\n",
    "                metrics=metrics,\n",
    "            )\n",
    "        return model\n",
    "\n",
    "    \n",
    "def _get_serve_tf_examples_fn(model, tf_transform_output):\n",
    "    \"\"\"Returns a function that parses a serialized tf.Example.\"\"\"\n",
    "\n",
    "    model.tft_layer = tf_transform_output.transform_features_layer()\n",
    "\n",
    "    @tf.function\n",
    "    def serve_tf_examples_fn(serialized_tf_examples):\n",
    "        \"\"\"Returns the output to be used in the serving signature.\"\"\"\n",
    "        feature_spec = tf_transform_output.raw_feature_spec()\n",
    "        feature_spec.pop('series_ep_tags')\n",
    "        \n",
    "        parsed_features = tf.io.parse_example(\n",
    "            serialized_tf_examples, feature_spec\n",
    "        )\n",
    "\n",
    "        transformed_features = model.tft_layer(parsed_features)\n",
    "\n",
    "        outputs = model(transformed_features)\n",
    "        return {\"outputs\": outputs}\n",
    "\n",
    "    return serve_tf_examples_fn\n",
    "\n",
    "\n",
    "def run_fn(fn_args):\n",
    "    \"\"\"Train the model based on given args\n",
    "    \n",
    "    Args:\n",
    "        fn_args: Holds args used to train the model as name/value pairs\n",
    "    \"\"\"\n",
    "    tf_transform_output = tft.TFTransformOutput(fn_args.transform_output)\n",
    "    \n",
    "    num_tags = tf_transform_output.vocabulary_size_by_name('tags')\n",
    "    tag_file = tf_transform_output.vocabulary_file_by_name('tags')\n",
    "    vocab_size = tf_transform_output.vocabulary_size_by_name('vocab')\n",
    "    vocab_file = tf_transform_output.vocabulary_file_by_name('vocab')\n",
    "    vocab_df = pd.read_csv(vocab_file, header=None)\n",
    "    \n",
    "    table = create_tag_lookup_table(tag_file)\n",
    "    \n",
    "    train_dataset = _input_fn(\n",
    "        file_pattern=fn_args.train_files,\n",
    "        tf_transform_output=tf_transform_output,\n",
    "        num_tags=num_tags,\n",
    "        table=table,\n",
    "        batch_size=64)\n",
    "\n",
    "    eval_dataset = _input_fn(\n",
    "        file_pattern=fn_args.eval_files,\n",
    "        tf_transform_output=tf_transform_output,\n",
    "        batch_size=64,\n",
    "        num_tags=num_tags,\n",
    "        table=table)\n",
    "    \n",
    "    model = AutoTaggingModel(\n",
    "        embedding_dim=300,\n",
    "        train_embedding=True,\n",
    "        embedding_file=EMBEDDING_LOCATION,\n",
    "        output_size=num_tags + 1,\n",
    "        vocab_size=vocab_size + 1,\n",
    "        vocab_df=vocab_df,\n",
    "        max_string_length=MAX_STRING_LENGTH).get_model()\n",
    "    \n",
    "    early_stopping_callback = callbacks.EarlyStopping(monitor='val_loss',\n",
    "        min_delta=0.0001,\n",
    "        patience=4,\n",
    "        verbose=0,  \n",
    "        mode='auto',  \n",
    "        restore_best_weights=True)  \n",
    "\n",
    "    reduce_lr = callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss', \n",
    "        factor=0.1, \n",
    "        patience=2, \n",
    "        verbose=0, \n",
    "        mode='auto',\n",
    "        min_delta=0.0001) \n",
    "    \n",
    "    history = model.fit(\n",
    "        train_dataset, \n",
    "        epochs=10, \n",
    "        steps_per_epoch=fn_args.train_steps / 10, \n",
    "        validation_data=eval_dataset,\n",
    "        validation_steps=fn_args.eval_steps,\n",
    "        callbacks=[early_stopping_callback, reduce_lr]\n",
    "    )\n",
    "    \n",
    "    signatures = {\n",
    "        \"serving_default\": _get_serve_tf_examples_fn(model, tf_transform_output).get_concrete_function(\n",
    "            tf.TensorSpec(shape=[None], dtype=tf.string, name=\"examples\")\n",
    "        ),\n",
    "    }\n",
    "    \n",
    "    model.save(\n",
    "        fn_args.serving_model_dir, save_format=\"tf\", signatures=signatures\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Task.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing trainer/task.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile trainer/task.py\n",
    "\n",
    "import argparse \n",
    "import os \n",
    "\n",
    "from trainer import model\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    \n",
    "    parser.add_argument(\n",
    "        \"--transform_output\",\n",
    "        dest=\"transform_output\",\n",
    "        help=\"Location of transform_fn directory\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--train_files\",\n",
    "        dest=\"train_files\",\n",
    "        help=\"Path to training files\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--eval_files\",\n",
    "        dest=\"eval_files\",\n",
    "        help=\"Path to evaluation files\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--job-dir\",\n",
    "        dest=\"job-dir\",\n",
    "        help=\"This model ignores this field, but it is required by gcloud\",\n",
    "        default=\"Junk\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--serving_model_dir\",\n",
    "        dest=\"serving_model_dir\",\n",
    "        help=\"GCS location to write checkpoitns and export models\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--train_steps\",\n",
    "        dest=\"train_steps\",\n",
    "        type=int,\n",
    "        help=\"Number of steps to train\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--eval_steps\",\n",
    "        dest=\"eval_steps\",\n",
    "        type=int,\n",
    "        help=\"Number of steps to eval\"\n",
    "    )\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    print(args)\n",
    "    model.run_fn(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-10-26 06:26:35.380766\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://ml-sandbox-tagging-tfx-experiments/new_model/201026_062637\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "DATE_SUFFIX=$(date -u +%y%m%d_%H%M%S)\n",
    "OUTDIR=gs://${BUCKET}/new_model/${DATE_SUFFIX}\n",
    "    \n",
    "echo ${OUTDIR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(eval_files='gs://ml-sandbox-tagging-tfx-experiments/transform_dir/test*', eval_steps=1, serving_model_dir='gs://ml-sandbox-tagging-tfx-experiments/new_model/201026_062653/', train_files='gs://ml-sandbox-tagging-tfx-experiments/transform_dir/train*', train_steps=10, transform_output='gs://ml-sandbox-tagging-tfx-experiments/transform_dir', **{'job-dir': 'gs://ml-sandbox-tagging-tfx-experiments/new_model/201026_062653/'})\n",
      "Epoch 1/10\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.6933 - precision: 0.0035 - recall: 0.4983 - val_loss: 0.6736 - val_precision: 0.0037 - val_recall: 0.2410\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6730 - precision: 0.0038 - recall: 0.2528 - val_loss: 0.6329 - val_precision: 0.0037 - val_recall: 0.0991\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6292 - precision: 0.0053 - recall: 0.1378 - val_loss: 0.5543 - val_precision: 0.0082 - val_recall: 0.0627\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.5502 - precision: 0.0056 - recall: 0.0529 - val_loss: 0.4283 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.4260 - precision: 0.0116 - recall: 0.0202 - val_loss: 0.2754 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.2732 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.1299 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.1299 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.0484 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0516 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.0246 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0265 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.0260 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0271 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.0294 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-10-26 06:27:03.884238: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2300000000 Hz\n",
      "2020-10-26 06:27:03.884696: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c648b54880 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2020-10-26 06:27:03.884740: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2020-10-26 06:27:03.887180: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n",
      "WARNING:tensorflow:There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "2020-10-26 06:28:07.673277: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "DATE_SUFFIX=$(date -u +%y%m%d_%H%M%S)\n",
    "OUTDIR=gs://${BUCKET}/new_model/${DATE_SUFFIX}/\n",
    "JOBID=example_model_${DATE_SUFFIX}\n",
    "\n",
    "gcloud ai-platform local train \\\n",
    "    --module-name=trainer.task \\\n",
    "    --package-path=trainer \\\n",
    "    --job-dir=${OUTDIR} \\\n",
    "    -- \\\n",
    "    --train_files=gs://${BUCKET}/transform_dir/train* \\\n",
    "    --eval_files=gs://${BUCKET}/transform_dir/test* \\\n",
    "    --transform_output=gs://${BUCKET}/transform_dir \\\n",
    "    --serving_model_dir=${OUTDIR} \\\n",
    "    --train_steps=10 \\\n",
    "    --eval_steps=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submit Job to AI Platform for Full Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing trainer/config.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile trainer/config.yaml\n",
    "trainingInput:\n",
    "  scaleTier: CUSTOM\n",
    "  masterType: n1-standard-16\n",
    "  masterConfig:\n",
    "    acceleratorConfig:\n",
    "      count: 1\n",
    "      type: NVIDIA_TESLA_T4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jobId: autotagging_201026_062943\n",
      "state: QUEUED\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Job [autotagging_201026_062943] submitted successfully.\n",
      "Your job is still active. You may view the status of your job with the command\n",
      "\n",
      "  $ gcloud ai-platform jobs describe autotagging_201026_062943\n",
      "\n",
      "or continue streaming the logs with the command\n",
      "\n",
      "  $ gcloud ai-platform jobs stream-logs autotagging_201026_062943\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "DATE_SUFFIX=$(date -u +%y%m%d_%H%M%S)\n",
    "OUTDIR=gs://${BUCKET}/new_model/${DATE_SUFFIX}/\n",
    "JOBID=autotagging_${DATE_SUFFIX}\n",
    "\n",
    "gcloud ai-platform jobs submit training ${JOBID} \\\n",
    "    --region=europe-west2 \\\n",
    "    --package-path=trainer \\\n",
    "    --module-name=trainer.task \\\n",
    "    --job-dir=gs://${BUCKET}/new_model_job_dir \\\n",
    "    --staging-bucket=gs://${BUCKET} \\\n",
    "    --config=trainer/config.yaml \\\n",
    "    --runtime-version=2.2 \\\n",
    "    --python-version=3.7 \\\n",
    "    -- \\\n",
    "    --train_files=gs://${BUCKET}/transform_dir/train* \\\n",
    "    --eval_files=gs://${BUCKET}/transform_dir/test* \\\n",
    "    --transform_output=gs://${BUCKET}/transform_dir \\\n",
    "    --serving_model_dir=${OUTDIR} \\\n",
    "    --train_steps=14000 \\\n",
    "    --eval_steps=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf trainer"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-2-3-gpu.2-3.m56",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-2-3-gpu.2-3:m56"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
