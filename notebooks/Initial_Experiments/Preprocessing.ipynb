{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Notebook\n",
    "------------------------\n",
    "\n",
    "The goal of this notebook is to discover whether the existing model (as found in `Glove_embedding.ipynb` can be written in such a way as to take advantage of the capabilities offered by TFX. In particular, we'd like to understand whether the preprocessing steps can be included in the model graph and deployed as a single artifact. In the event that any preprocessing steps change, we'd like to confirm that model performance isn't negatively impacted. The overall goal is to replicate the behavior of the existing model as best as possible while simplifying productionization and operationalization. \n",
    "\n",
    "The notebook is structured as follows: \n",
    "\n",
    " **A. Pre-preprocessing:** Exact duplication of steps found in `Glove_embedding.ipynb`. Eventually these steps, as well as the steps found in earlier notebooks, should be made part of a robust data pipeline in order to facilitate on-demand training of new models on appropriate data.\n",
    " \n",
    " **B. Preprocessing:** Ensure that we can replicate existing preprocessing steps in tensorflow. These steps would be included in the `preprocessing_fn` used by the Transform component in a TFX pipeline. \n",
    " \n",
    "**NOTE:** Much of the code included in this notebook (e.g. explicit calls to `tft_beam`) is handled under the hood when using TFX. The thinking behind including this code as opposed to diving straight in with an end-to-end TFX pipeline is twofold: \n",
    "\n",
    " - Ensure that any changes to the code don't negatively impact the model \n",
    " - Give an idea of the types of things happening under the hood when using TFX. Hopefully this will help to demystify and facilitate debugging in the future. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_transform as tft\n",
    "import tensorflow_transform.beam as tft_beam\n",
    "from tensorflow_transform.tf_metadata import dataset_metadata\n",
    "from tensorflow_transform.tf_metadata import schema_utils\n",
    "import apache_beam as beam\n",
    "\n",
    "import tempfile\n",
    "import pprint\n",
    "import apache_beam as beam\n",
    "\n",
    "import os\n",
    "\n",
    "import tensorflow as tf \n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from google.cloud import storage\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For some reason I needed to install this when using AI platform notebooks. Not really sure why? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --user google-resumable-media==0.6.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "### **A. Pre-preprocessing**\n",
    "\n",
    "The following cells simply replicate some of first steps in the `Glove_embedding.ipynb` notebook. Ideally these steps would come at the end of a data processing pipeline with the result being clean csv files that could then be picked up the TFX training pipeline. \n",
    "\n",
    "**NOTE:** Exactly where the \"pre-preprocessing\"/data pipeline ends and the training pipeline begins is currently a bit unclear and should be discussed and agreed upon by the Data Scientists and ML Engineers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "translated = pd.read_csv(\"gs://ml-sandbox-101-tagging/data/processed/translation_data/translated_data_entode.csv\", index_col=0)\n",
    "overlap = pd.read_csv(\"gs://ml-sandbox-101-tagging/data/processed/de_gb_overlap_data.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/pandas/core/frame.py:4303: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  errors=errors,\n"
     ]
    }
   ],
   "source": [
    "overlap.rename(columns={\"combi\":\"full_translated\"}, inplace=True)\n",
    "overlap = pd.merge(translated,overlap,how='left',on='TitleId')\n",
    "\n",
    "overlap['features'] = overlap[['full_translated_x','full_translated_y','series_ep_subgenre_y']].fillna(\"\").apply(\" \".join, axis=1)\n",
    "train_data_all =  overlap[['TitleId', 'series_ep_uuid_x', 'title_x', 'series_ep_title_x',\n",
    "       'series_name_x', 'series_ep_synopsis_x', 'genre_x',\n",
    "       'series_ep_subgenre_x', 'series_ep_tags_x','translated_text',\n",
    "       'tranlsated_subgenres', 'tranlsated_tags',  'features']]\n",
    "\n",
    "train_data_all.rename(columns={'series_ep_uuid_x':'series_ep_uuid',\n",
    "                               'title_x':'title', \n",
    "                               'series_ep_title_x':'series_ep_title',\n",
    "                               'series_name_x':'series_name', \n",
    "                               'series_ep_synopsis_x':'series_ep_synopsis', \n",
    "                               'genre_x':'genre',\n",
    "                               'series_ep_subgenre_x':'series_ep_subgenre', \n",
    "                               'series_ep_tags_x':'series_ep_tags'},inplace=True)\n",
    "\n",
    "train_data_all = train_data_all[train_data_all.features.apply(len)>30]\n",
    "\n",
    "# split train/test uuids\n",
    "train_uuids,test_uuids =train_test_split(train_data_all.series_ep_uuid.unique(), shuffle=True, test_size=0.1, random_state=42)\n",
    "# We take a series and we want it all to go in training or all in testing avoid any possible leakage\n",
    "\n",
    "train_data = train_data_all.loc[train_data_all.series_ep_uuid.isin(train_uuids),:].copy()\n",
    "test_data = train_data_all[train_data_all.series_ep_uuid.isin(test_uuids)].copy()\n",
    "\n",
    "train_data['series_ep_tags'] = train_data['series_ep_tags'].apply(eval)\n",
    "test_data['series_ep_tags'] = test_data['series_ep_tags'].apply(eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>features</th>\n",
       "      <th>series_ep_tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ein amerikanischer Militärbeamter untersucht d...</td>\n",
       "      <td>[20th century, thriller, the middle east, myst...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Aufschlussreiche Dokumentation über den Weltme...</td>\n",
       "      <td>[sport skills &amp; tricks, bio, mental health, ad...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ein Söldnerteam wird von der CIA beauftragt, n...</td>\n",
       "      <td>[thriller, military personnel, soldier, action...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ein Amerikaner lässt sich in einem Dorf im feu...</td>\n",
       "      <td>[the far east, thriller, china, 19th century, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Ester und ihre Mutter führen ein scheinbar bes...</td>\n",
       "      <td>[criminal, usa, latin america, violent, thrill...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99118</th>\n",
       "      <td>Irinas Schicksal zwingt Georgina, sich dem Sch...</td>\n",
       "      <td>[wealthy family, thriller, crime, mystery, wea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99119</th>\n",
       "      <td>Ein Sky-Original mit Lennie James. Ne&amp;#39;er-d...</td>\n",
       "      <td>[gritty, thriller, urban, crime, london, famil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99120</th>\n",
       "      <td>Ellie und Gedeon stoßen zusammen, als die Ermi...</td>\n",
       "      <td>[german language, germany, gritty, thriller, c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99121</th>\n",
       "      <td>Kollateralschaden: McNultys Neugier lässt die ...</td>\n",
       "      <td>[usa, gritty, crime, hbo, police, tense, corru...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99122</th>\n",
       "      <td>Die Polizei glaubt, einen Vorsprung zu haben, ...</td>\n",
       "      <td>[criminal, gritty, thriller, crime, mystery, m...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>90543 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                features  \\\n",
       "0      Ein amerikanischer Militärbeamter untersucht d...   \n",
       "1      Aufschlussreiche Dokumentation über den Weltme...   \n",
       "2      Ein Söldnerteam wird von der CIA beauftragt, n...   \n",
       "4      Ein Amerikaner lässt sich in einem Dorf im feu...   \n",
       "5      Ester und ihre Mutter führen ein scheinbar bes...   \n",
       "...                                                  ...   \n",
       "99118  Irinas Schicksal zwingt Georgina, sich dem Sch...   \n",
       "99119  Ein Sky-Original mit Lennie James. Ne&#39;er-d...   \n",
       "99120  Ellie und Gedeon stoßen zusammen, als die Ermi...   \n",
       "99121  Kollateralschaden: McNultys Neugier lässt die ...   \n",
       "99122  Die Polizei glaubt, einen Vorsprung zu haben, ...   \n",
       "\n",
       "                                          series_ep_tags  \n",
       "0      [20th century, thriller, the middle east, myst...  \n",
       "1      [sport skills & tricks, bio, mental health, ad...  \n",
       "2      [thriller, military personnel, soldier, action...  \n",
       "4      [the far east, thriller, china, 19th century, ...  \n",
       "5      [criminal, usa, latin america, violent, thrill...  \n",
       "...                                                  ...  \n",
       "99118  [wealthy family, thriller, crime, mystery, wea...  \n",
       "99119  [gritty, thriller, urban, crime, london, famil...  \n",
       "99120  [german language, germany, gritty, thriller, c...  \n",
       "99121  [usa, gritty, crime, hbo, police, tense, corru...  \n",
       "99122  [criminal, gritty, thriller, crime, mystery, m...  \n",
       "\n",
       "[90543 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(train_data[['features', 'series_ep_tags']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since these are really the only two columns we need for training the model, it would be nice to have these as outputs from the data pipeline and inputs to the training pipeline. \n",
    "\n",
    "Some work will be needed to understand how data comes into the service. Any preprocessing steps to get data into the above format (without the tags) could possibly be included the serving_input_function we export with the model. If any of the above pre-preprocessing steps are necessary for the serving data as well, these steps could be moved into the preprocessing steps of the training pipeline. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "### **B. Preprocessing**\n",
    "\n",
    "**Can we replicate existing preprocessing steps?**\n",
    "\n",
    "The bulk of the preprocessing consisted of a few steps: \n",
    " 1. Binarizing the labels to get a multi-one-hot encoding (using Sklearn's `MultiLabelBinarizer`)\n",
    " 2. Cleaning the text by removing punctuation and stop words followed by stemming (using a german-language model from Spacy as well as regex methods in python)\n",
    " 3. Tokenizing the text (using Keras tokenizer) \n",
    " \n",
    "A major focus in ML Engineering is **consistent** preprocessing so as to avoid training-serving skew. Leveraging a library such as tensorflow transform allows us to build our preprocessing steps as TF graphs that can be deployed together with our model as a single artifact. This allows a requesting client to submit raw data and then the preprocessing happens on the deployed model graph. \n",
    "\n",
    "In order to take advantage of this capability one must define a **preprocessing function** which consists of only regular tensorflow operations and/or **analyzers** provided by tf.Transform. Analyzers cause tf.Transform to compute a **full-pass** operation outside of tensorflow and subsequently use a generated constant tensor in the preprocessing graph. For instance, there are analyzers to compute the minimum and maximum of a dataset. The generated constants from these analyzers can the be built into the model graph to do things like min-max scaling of incoming data based on statistics from the training data.  \n",
    "\n",
    "---\n",
    "The main question we are trying to answer here is whether we can replicate the existing preprocessing steps in a manner which uses only tensorflow operations, without negatively impacting model performance. Unfortunately, this requires us to rethink some of the preprocessing steps listed above; however, this also provides us an opportunity to streamline and improve these steps as well. \n",
    "\n",
    "In particular, it is worth rethinking the use of the Spacy language model since this would be quite difficult to replicate using Tensorflow operations. It is important to keep in mind that we are doing text preprocessing in order to embed tokens using pre-trained Glove embeddings. We found some resources that indicated it was in fact **unnecessary and perhaps even counter-productive to perform complex preprocessing such as stopword removal and stemming:**\n",
    " 1. [This paper](https://arxiv.org/pdf/1707.01780.pdf) is perhaps the most insightful \n",
    " \n",
    "     ```\n",
    "      \"Our evaluation highlights the importance of being consistent in the preprocessing strategy employed across training and evaluation data. In general a simple tokenized corpus works equally or better than more complex preprocessing techniques such as lemmatization or multiword grouping, except for a dataset corresponding to a specialized domain, like health, in which sole tokenization performs poorly. Addi- tionally, word embeddings trained on multiword- grouped corpora perform surprisingly well when applied to simple tokenized datasets.”\n",
    "     ```\n",
    "     \n",
    "     \n",
    " 2. [This kaggle kernel](https://www.kaggle.com/christofhenkel/how-to-preprocessing-when-using-embeddings) is also useful\n",
    " \n",
    "     ```\n",
    "     1. Don't use standard preprocessing steps like stemming or stopword removal when you have pre-trained embeddings Some of you might used standard preprocessing steps when doing word count based feature extraction (e.g. TFIDF) such as removing stopwords, stemming etc. The reason is simple: You lose valuable information, which would help your NN to figure things out.\n",
    "     2. Get your vocabulary as close to the embeddings as possible I will focus in this notebook, how to achieve that. For an example I take the GoogleNews pretrained embeddings, there is no deeper reason for this choice.\"\n",
    "     ```\n",
    "     \n",
    "     \n",
    " 3. Finally [this resource](https://mlwhiz.com/blog/2019/01/17/deeplearning_nlp_preprocess/) does a good job summarizing the main point: \n",
    " \n",
    "     ```\n",
    "     In principle our preprocessing should match the preprocessing that was used before training the word embedding\n",
    "     ```\n",
    "\n",
    "The main takeaway is that methods such as stopword removal and lemmatization are less relevant for modern-day deep learning NLP techniques. This is because many such algorithms are explicitly trained to consider the context of a word—context which is lost after doing things like stopword removal. \n",
    "\n",
    "From what we could find, it looks like GloVe doesn't do any stop word removal or lemmatization and therefore we might be able to get away without doing either. This allows us to focus on recreating the other preprocessing steps, in particular **cleaning of punctuation and generating a vocabulary for the text**. The ultimate, test however, is how much of our vocabulary is covered by the pretrained embeddings. In the `glove_embedding.ipynb` notebook we see that we could get about **66% coverage** of the vocabulary. **We must ensure that our preprocessing steps do not cause coverage do be any lower than this.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'features': 'Ein amerikanischer Militärbeamter untersucht den Tod einer Hubschrauberpilotin im Golfkrieg von 1990, um zu entscheiden, ob ihre Handlungen eine posthume Ehrenmedaille verdienen. In der Zwischenzeit versucht er, sich mit einem tragischen Vorfall auseinanderzusetzen, den er während des Konflikts selbst verursacht und vertuscht hat. Drama mit Denzel Washington, Meg Ryan, Lou Diamond Phillips, Scott Glenn und Matt Damon. .. Krieg Theater. Courage Under Fire  ',\n",
       " 'series_ep_tags': ['20th century',\n",
       "  'thriller',\n",
       "  'the middle east',\n",
       "  'mystery',\n",
       "  'gulf war',\n",
       "  'war',\n",
       "  '1990s',\n",
       "  'ethics & morality',\n",
       "  'military personnel',\n",
       "  'action',\n",
       "  'drama',\n",
       "  'military',\n",
       "  'officer-colonel']}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_records = train_data[['features', 'series_ep_tags']].to_dict('records')\n",
    "test_data_records = test_data[['features', 'series_ep_tags']].to_dict('records')\n",
    "\n",
    "train_data_records[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### **B.1 Regex**\n",
    "\n",
    "The text cleanup class includes methods to remove punctuation, special characters and numbers, and to filter out short words. Interestingly, this text cleanup was followed by a [keras tokenizer](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer), which actually filters out punctuation and special characters as well. Instead of filtering out these characters (twice), why not instead \"whitelist\" the characters we want to keep. Specifically, we could just keep letters that we want (alphabet plus german characters **äÄöÖüÜß**)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Während einer harten Afghanistan-Tour findet der gewissenhafte französische Armeekapitän Bonassieu (Jeremie Renier) seine Autorität im Stich gelassen, als seine Männer unerklärlich verschwinden. Er vermutet natürlich, dass es die Arbeit der Taliban ist ... aber es stellt sich heraus, dass sie auf unheimlich ähnliche Weise Truppen verloren haben. Ein unsichtbarer Feind macht sich in diesem nervenden psychologischen Kriegsthriller bemerkbar.. Weltkino Theater. Neither Heaven Nor Earth  \n"
     ]
    }
   ],
   "source": [
    "white_list = '[^äÄöÖüÜßa-zA-Z]'\n",
    "\n",
    "text_sample = train_data_records[25000]['features']\n",
    "\n",
    "print(text_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'W\\xc3\\xa4hrend einer harten Afghanistan Tour findet der gewissenhafte franz\\xc3\\xb6sische Armeekapit\\xc3\\xa4n Bonassieu  Jeremie Renier  seine Autorit\\xc3\\xa4t im Stich gelassen  als seine M\\xc3\\xa4nner unerkl\\xc3\\xa4rlich verschwinden  Er vermutet nat\\xc3\\xbcrlich  dass es die Arbeit der Taliban ist     aber es stellt sich heraus  dass sie auf unheimlich \\xc3\\xa4hnliche Weise Truppen verloren haben  Ein unsichtbarer Feind macht sich in diesem nervenden psychologischen Kriegsthriller bemerkbar   Weltkino Theater  Neither Heaven Nor Earth  ', shape=(), dtype=string)\n",
      "CPU times: user 24 ms, sys: 0 ns, total: 24 ms\n",
      "Wall time: 22.2 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "cleaned = tf.strings.regex_replace(text_sample, white_list, ' ')\n",
    "\n",
    "print(cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This seems to do what we want to do; however, the next step was to remove short words. \n",
    "\n",
    "Using word boundaries doesn't work with german characters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 943 µs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=string, numpy=b' hrend einer harten Afghanistan Tour findet   gewissenhafte franz sische Armeekapit  Bonassieu  Jeremie Renier  seine Autorit    Stich gelassen    seine  nner unerkl rlich verschwinden    vermutet   rlich  dass     Arbeit   Taliban       aber   stellt sich heraus  dass     unheimlich \\xc3\\xa4hnliche Weise Truppen verloren haben    unsichtbarer Feind macht sich   diesem nervenden psychologischen Kriegsthriller bemerkbar   Weltkino Theater  Neither Heaven   Earth  '>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "tf.strings.regex_replace(cleaned, r'\\b[äÄöÖüÜßa-zA-Z]{1,3}\\b', ' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notice the first word (should be während)**\n",
    "\n",
    "This solution from John Pawley seems to work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 982 µs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=string, numpy=b'W\\xc3\\xa4hrend einer harten Afghanistan Tour findet gewissenhafte franz\\xc3\\xb6sische Armeekapit\\xc3\\xa4n Bonassieu  Jeremie Renier  seine Autorit\\xc3\\xa4t Stich gelassen  seine M\\xc3\\xa4nner unerkl\\xc3\\xa4rlich verschwinden  vermutet nat\\xc3\\xbcrlich  dass Arbeit Taliban     aber stellt sich heraus  dass unheimlich \\xc3\\xa4hnliche Weise Truppen verloren haben  unsichtbarer Feind macht sich diesem nervenden psychologischen Kriegsthriller bemerkbar   Weltkino Theater  Neither Heaven Earth  '>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "tf.strings.regex_replace(cleaned, r'((^|\\s)[äÄöÖüÜßa-zA-Z]{1,3})+\\s', ' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considering we are no longer removing stop words, it perhaps makes sense to limit the filtering to 1/2 letter words as opposed to filtering out all words length 3 or less. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### **B.2 `preprocessing_fn`**\n",
    "\n",
    "Now we can write a simple preprocessing function called `preprocessing_fn`. This function will be found and used by the `Transform` component in TFX to construct the preprocessing pipeline. In the preprocessing_fn we can make heavy use of Tensorflow Transform for performing feature engineering on the dataset, especially for any engineering that requires a full-pass over the dataset (e.g. vocabulary generation). \n",
    "\n",
    "**How is this different from transforming features inside modelling code?** Using feature columns in model code, one can also do some simple feature engineering. The key is that these transformations can be defined without looking at the data. TFT, on the other hand, is useful when one must perform a full pass over the data. In addition, by transforming the data ahead of time we can potentially accelerate the training process. \n",
    "\n",
    "See [this page](https://www.tensorflow.org/tfx/guide/transform#transform_and_tensorflow_transform) for more information. \n",
    "\n",
    "---\n",
    "\n",
    "The `preprocessing_fn` describes a series of operations on Tensorflow tensors. The input to the function is determined by a Schema proto containing a list of features. \n",
    "\n",
    "In this particular example we will have two inputs, the synopsis (the text that constitutes the input feature to the model) and the tags (the label(s) to be predicted). Our preprocessing function will need to clean and tokenize the text. We will then compute and apply vocabularies to both the feature and the tags in order to convert strings to integers. \n",
    "\n",
    "---\n",
    "**NOTE:** In this particular example we make some simplifications. Namely, we hard-code a predefined a `MAX_STRING_LENGTH` and a number of unique tags. This allows us to do the appropriate padding and create a multi-one-hot encoded vector for the tags. In the future, we may want to have these values as outputs of the data pre-processing pipeline or find a better way to do this. See [this stackoverflow question](https://stackoverflow.com/questions/59793174/how-to-use-tf-transform-analyzer-variables-in-preprocessing-fn).  \n",
    "\n",
    "**NOTE:** Again, extra detail is included to give some detail about what is happening under the hood when using the transform component. See [this example](https://github.com/tensorflow/transform/blob/master/examples/sentiment_example.py) of using tensorflow transform in a standalone manner vs. [the `preprocessing_fn` in this example](https://github.com/tensorflow/tfx/blob/master/tfx/examples/chicago_taxi_pipeline/taxi_utils_native_keras.py) to understand the difference. \n",
    "\n",
    "**NOTE:** We had issues when serving models with the transform layer when using `tft.compute_and_apply_vocabulary` on our label data. When removing this label field from the input schema spec for serving there were issues with the graph requiring the label field. Seems there is something going on with compute_and_apply_vocabulary that requires the field to be present all the time. Therefore we only compute the vocab in preprocessing and apply it when generating training/validation samples, this ensure the fields can be ignored within the serving graph.\n",
    "\n",
    "**NOTE:** Would like to be able to use analyzers in the preprocessor function, similar to [this question on stackoverflow](https://stackoverflow.com/questions/59793174/how-to-use-tf-transform-analyzer-variables-in-preprocessing-fn). Unfortunately that means that we can't do things like the following and that we may have to hard-code the MAX_STRING_LENGTH to a reasonable value (currently use the maximum string length in the training set, in the future could just set this to something like 200): \n",
    "```python\n",
    "num_tokens = tft.word_count(text_tokens)\n",
    "max_len = tft.max(num_tokens)\n",
    "text_tokens = text_tokens.to_tensor(shape=[None, MAX_STRING_LENGTH])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYNOPSIS = 'features'\n",
    "TAGS = 'series_ep_tags'\n",
    "MAX_STRING_LENGTH = 277\n",
    "\n",
    "white_list = '[^äÄöÖüÜßa-zA-Z]'\n",
    "\n",
    "def clean(text):\n",
    "    \"\"\"\n",
    "    Clean up text input by removing everything but the \n",
    "    white-listed characters\n",
    "    \"\"\"\n",
    "    # Encoding needed to keep german characters\n",
    "    lower = tf.strings.lower(text, 'utf-8')\n",
    "    cleaned = tf.strings.regex_replace(lower, white_list, ' ')\n",
    "    cleaned = tf.strings.strip(cleaned)\n",
    "    # Filter single letters\n",
    "    clean_1 = tf.strings.regex_replace(cleaned, r'((^|\\s)[äÄöÖüÜßa-zA-Z]{1})+\\s', ' ')\n",
    "    # Replace multiple spaces with single space\n",
    "    final = tf.strings.regex_replace(clean_1, ' +', ' ')\n",
    "\n",
    "    return final\n",
    "\n",
    "def preprocessing_fn(inputs):\n",
    "    \"\"\"Preprocess input columns into transformed columns.\"\"\"\n",
    "    \n",
    "    text = inputs[SYNOPSIS]\n",
    "    tags = inputs[TAGS]\n",
    "\n",
    "    cleaned = clean(text)\n",
    "    text_tokens = tf.compat.v1.string_split(cleaned, ' ', result_type='RaggedTensor')\n",
    "\n",
    "    text_tokens = text_tokens.to_tensor(shape=[None, MAX_STRING_LENGTH])\n",
    "    text_indices = tft.compute_and_apply_vocabulary(\n",
    "        text_tokens, vocab_filename='vocab', num_oov_buckets=1\n",
    "    )\n",
    "    \n",
    "    # compute vocab of tags, do not apply due to serving issues\n",
    "    _ = tft.vocabulary(tags, vocab_filename='tags')\n",
    "    \n",
    "    # Need to transform the name\n",
    "    return {\n",
    "        SYNOPSIS: text_indices,\n",
    "        TAGS: tags\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** The next feels cells take quite a long time (~10 min)! Can skip if the data is already in GCS. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir transform_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Tensorflow version (2.3.0) found. Note that Tensorflow Transform support for TF 2.0 is currently in beta, and features such as tf.function may not work as intended. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:apache_beam.runners.interactive.interactive_environment:Dependencies required for Interactive Beam PCollection visualization are not available, please use: `pip install apache-beam[interactive]` to install necessary dependencies to enable all data visualization features.\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (typeof window.interactive_beam_jquery == 'undefined') {\n",
       "          var jqueryScript = document.createElement('script');\n",
       "          jqueryScript.src = 'https://code.jquery.com/jquery-3.4.1.slim.min.js';\n",
       "          jqueryScript.type = 'text/javascript';\n",
       "          jqueryScript.onload = function() {\n",
       "            var datatableScript = document.createElement('script');\n",
       "            datatableScript.src = 'https://cdn.datatables.net/1.10.20/js/jquery.dataTables.min.js';\n",
       "            datatableScript.type = 'text/javascript';\n",
       "            datatableScript.onload = function() {\n",
       "              window.interactive_beam_jquery = jQuery.noConflict(true);\n",
       "              window.interactive_beam_jquery(document).ready(function($){\n",
       "                \n",
       "              });\n",
       "            }\n",
       "            document.head.appendChild(datatableScript);\n",
       "          };\n",
       "          document.head.appendChild(jqueryScript);\n",
       "        } else {\n",
       "          window.interactive_beam_jquery(document).ready(function($){\n",
       "            \n",
       "          });\n",
       "        }"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        var import_html = () => {\n",
       "          ['https://raw.githubusercontent.com/PAIR-code/facets/1.0.0/facets-dist/facets-jupyter.html'].forEach(href => {\n",
       "            var link = document.createElement('link');\n",
       "            link.rel = 'import'\n",
       "            link.href = href;\n",
       "            document.head.appendChild(link);\n",
       "          });\n",
       "        }\n",
       "        if ('import' in document.createElement('link')) {\n",
       "          import_html();\n",
       "        } else {\n",
       "          var webcomponentScript = document.createElement('script');\n",
       "          webcomponentScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/webcomponentsjs/1.3.3/webcomponents-lite.js';\n",
       "          webcomponentScript.type = 'text/javascript';\n",
       "          webcomponentScript.onload = function(){\n",
       "            import_html();\n",
       "          };\n",
       "          document.head.appendChild(webcomponentScript);\n",
       "        }"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Tensorflow version (2.3.0) found. Note that Tensorflow Transform support for TF 2.0 is currently in beta, and features such as tf.function may not work as intended. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Tensorflow version (2.3.0) found. Note that Tensorflow Transform support for TF 2.0 is currently in beta, and features such as tf.function may not work as intended. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow_transform/tf_utils.py:218: Tensor.experimental_ref (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use ref() instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow_transform/tf_utils.py:218: Tensor.experimental_ref (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use ref() instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow/python/saved_model/signature_def_utils_impl.py:201: build_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.utils.build_tensor_info or tf.compat.v1.saved_model.build_tensor_info.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow/python/saved_model/signature_def_utils_impl.py:201: build_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.utils.build_tensor_info or tf.compat.v1.saved_model.build_tensor_info.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets added to graph.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets added to graph.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:No assets to write.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:No assets to write.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Issue encountered when serializing tft_mapper_use.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
      "'Counter' object has no attribute 'name'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Issue encountered when serializing tft_mapper_use.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
      "'Counter' object has no attribute 'name'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Issue encountered when serializing tft_analyzer_use.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
      "'Counter' object has no attribute 'name'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Issue encountered when serializing tft_analyzer_use.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
      "'Counter' object has no attribute 'name'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:SavedModel written to: /tmp/tmph62ndpdd/tftransform_tmp/01b0f5a1f8e0469eaef468edde942406/saved_model.pb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:SavedModel written to: /tmp/tmph62ndpdd/tftransform_tmp/01b0f5a1f8e0469eaef468edde942406/saved_model.pb\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets added to graph.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets added to graph.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:No assets to write.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:No assets to write.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Issue encountered when serializing tft_mapper_use.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
      "'Counter' object has no attribute 'name'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Issue encountered when serializing tft_mapper_use.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
      "'Counter' object has no attribute 'name'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Issue encountered when serializing tft_analyzer_use.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
      "'Counter' object has no attribute 'name'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Issue encountered when serializing tft_analyzer_use.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
      "'Counter' object has no attribute 'name'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:SavedModel written to: /tmp/tmph62ndpdd/tftransform_tmp/a560af43fbea40f0aea36cad35c3d463/saved_model.pb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:SavedModel written to: /tmp/tmph62ndpdd/tftransform_tmp/a560af43fbea40f0aea36cad35c3d463/saved_model.pb\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Tensorflow version (2.3.0) found. Note that Tensorflow Transform support for TF 2.0 is currently in beta, and features such as tf.function may not work as intended. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Tensorflow version (2.3.0) found. Note that Tensorflow Transform support for TF 2.0 is currently in beta, and features such as tf.function may not work as intended. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets added to graph.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets added to graph.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmph62ndpdd/tftransform_tmp/9e4398889ed045a7aeff053a991c39bf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmph62ndpdd/tftransform_tmp/9e4398889ed045a7aeff053a991c39bf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:SavedModel written to: /tmp/tmph62ndpdd/tftransform_tmp/9e4398889ed045a7aeff053a991c39bf/saved_model.pb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:SavedModel written to: /tmp/tmph62ndpdd/tftransform_tmp/9e4398889ed045a7aeff053a991c39bf/saved_model.pb\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
      "value: \"\\n\\013\\n\\tConst_1:0\\022\\005vocab\"\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
      "value: \"\\n\\013\\n\\tConst_1:0\\022\\005vocab\"\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
      "value: \"\\n\\013\\n\\tConst_3:0\\022\\004tags\"\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
      "value: \"\\n\\013\\n\\tConst_3:0\\022\\004tags\"\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
      "value: \"\\n\\013\\n\\tConst_1:0\\022\\005vocab\"\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
      "value: \"\\n\\013\\n\\tConst_1:0\\022\\005vocab\"\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
      "value: \"\\n\\013\\n\\tConst_3:0\\022\\004tags\"\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
      "value: \"\\n\\013\\n\\tConst_3:0\\022\\004tags\"\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Tensorflow version (2.3.0) found. Note that Tensorflow Transform support for TF 2.0 is currently in beta, and features such as tf.function may not work as intended. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Tensorflow version (2.3.0) found. Note that Tensorflow Transform support for TF 2.0 is currently in beta, and features such as tf.function may not work as intended. \n",
      "WARNING:apache_beam.utils.interactive_utils:Failed to alter the label of a transform with the ipython prompt metadata. Cannot figure out the pipeline that the given pvalueish ((<PCollection[CreatePInput0/Map(decode).None] at 0x7f90b6ae7350>, {'_schema': feature {\n",
      "  name: \"features\"\n",
      "  type: BYTES\n",
      "  presence {\n",
      "    min_fraction: 1.0\n",
      "  }\n",
      "  shape {\n",
      "  }\n",
      "}\n",
      "feature {\n",
      "  name: \"series_ep_tags\"\n",
      "  type: BYTES\n",
      "}\n",
      "}), (<PCollection[CreatePInput1/Map(decode).None] at 0x7f906d898a90>, BeamDatasetMetadata(dataset_metadata={'_schema': feature {\n",
      "  name: \"features\"\n",
      "  type: INT\n",
      "  int_domain {\n",
      "    is_categorical: true\n",
      "  }\n",
      "  presence {\n",
      "    min_fraction: 1.0\n",
      "  }\n",
      "  shape {\n",
      "    dim {\n",
      "      size: 277\n",
      "    }\n",
      "  }\n",
      "}\n",
      "feature {\n",
      "  name: \"series_ep_tags\"\n",
      "  type: BYTES\n",
      "}\n",
      "}, deferred_metadata=<PCollection[CreatePInput2/Map(decode).None] at 0x7f906d898250>))) belongs to. Thus noop.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
      "value: \"\\n\\013\\n\\tConst_1:0\\022\\005vocab\"\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
      "value: \"\\n\\013\\n\\tConst_1:0\\022\\005vocab\"\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
      "value: \"\\n\\013\\n\\tConst_3:0\\022\\004tags\"\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
      "value: \"\\n\\013\\n\\tConst_3:0\\022\\004tags\"\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 18s, sys: 4.26 s, total: 2min 23s\n",
      "Wall time: 2min 23s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# This schema would be produced automatically when running the \n",
    "# TFX pipeline\n",
    "raw_data_metadata = dataset_metadata.DatasetMetadata(\n",
    "    schema_utils.schema_from_feature_spec(\n",
    "        {\n",
    "            SYNOPSIS: tf.io.FixedLenFeature([], tf.string),\n",
    "            TAGS: tf.io.VarLenFeature(tf.string),\n",
    "        }\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "with tft_beam.Context(temp_dir=tempfile.mkdtemp()):\n",
    "    # Analyze and transform training data\n",
    "    transformed_dataset, transform_fn = (train_data_records, raw_data_metadata,) | tft_beam.AnalyzeAndTransformDataset(preprocessing_fn)\n",
    "    transformed_data, transformed_metadata = transformed_dataset \n",
    "\n",
    "    # Transform test data using transform_fn \n",
    "    raw_test_dataset = (test_data_records, raw_data_metadata)\n",
    "    transformed_test_dataset = (\n",
    "      (raw_test_dataset, transform_fn) | tft_beam.TransformDataset())\n",
    "    transformed_test_data, _ = transformed_test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:apache_beam.utils.interactive_utils:Failed to alter the label of a transform with the ipython prompt metadata. Cannot figure out the pipeline that the given pvalueish {'_schema': feature {\n",
      "  name: \"features\"\n",
      "  type: BYTES\n",
      "  presence {\n",
      "    min_fraction: 1.0\n",
      "  }\n",
      "  shape {\n",
      "  }\n",
      "}\n",
      "feature {\n",
      "  name: \"series_ep_tags\"\n",
      "  type: BYTES\n",
      "}\n",
      "} belongs to. Thus noop.\n"
     ]
    }
   ],
   "source": [
    "# needs beam pipeline to save the meta data\n",
    "with beam.Pipeline() as pipeline:\n",
    "    with tft_beam.Context(temp_dir=tempfile.mkdtemp()):\n",
    "        _ = (raw_data_metadata | 'WriteMetadata' >> tft_beam.WriteMetadata('./transform_dir/metadata', pipeline=pipeline))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:apache_beam.io.tfrecordio:Couldn't find python-snappy so the implementation of _TFRecordUtil._masked_crc32c is not as fast as it could be.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7min 26s, sys: 4.44 s, total: 7min 30s\n",
      "Wall time: 7min 31s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with tft_beam.Context(temp_dir=tempfile.mkdtemp()):\n",
    "    transformed_data_coder = tft.coders.ExampleProtoCoder(transformed_metadata.schema)\n",
    "    # Write out transformed training data\n",
    "    _ = (\n",
    "        transformed_data \n",
    "        | 'EncodeTrainData' >> beam.Map(transformed_data_coder.encode)\n",
    "        | 'WriteTrainData' >> beam.io.WriteToTFRecord(\n",
    "            os.path.join('./transform_dir', 'train_transformed'))\n",
    "    )\n",
    "\n",
    "    # Write out transformed test data\n",
    "    _ = (\n",
    "        (transformed_test_data \n",
    "         | 'EncodeTestData' >> beam.Map(transformed_data_coder.encode)\n",
    "         | 'WriteTestData' >> beam.io.WriteToTFRecord(\n",
    "             os.path.join('./transform_dir', 'test_transformed')))\n",
    "    )\n",
    "\n",
    "    # Write out the transform fn \n",
    "    _ = (\n",
    "        transform_fn\n",
    "        | 'WriteTransformFn' >> tft_beam.WriteTransformFn('./transform_dir')\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Took a very long time to write out examples?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file://transform_dir/test_transformed-00000-of-00001 [Content-Type=application/octet-stream]...\n",
      "Copying file://transform_dir/train_transformed-00000-of-00001 [Content-Type=application/octet-stream]...\n",
      "Copying file://transform_dir/transform_fn/assets/tags [Content-Type=application/octet-stream]...\n",
      "Copying file://transform_dir/transform_fn/saved_model.pb [Content-Type=application/octet-stream]...\n",
      "Copying file://transform_dir/transform_fn/assets/vocab [Content-Type=application/octet-stream]...\n",
      "Copying file://transform_dir/metadata/schema.pbtxt [Content-Type=application/octet-stream]...\n",
      "Copying file://transform_dir/transformed_metadata/schema.pbtxt [Content-Type=application/octet-stream]...\n",
      "\\ [7/7 files][ 84.3 MiB/ 84.3 MiB] 100% Done                                    \n",
      "Operation completed over 7 objects/84.3 MiB.                                     \n"
     ]
    }
   ],
   "source": [
    "!gsutil -m cp -r transform_dir gs://ml-sandbox-tagging-tfx-experiments/preprocessing_notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### **B.3 Preprocessing Analysis**\n",
    "\n",
    "Let's check out the generated vocabulary and the transformed datasets to see if they make sense!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How we can access the vocab and tag files \n",
    "tf_transform_output = tft.TFTransformOutput('gs://ml-sandbox-tagging-tfx-experiments/preprocessing_notebook/')\n",
    "\n",
    "VOCAB_FILE = tf_transform_output.vocabulary_file_by_name('vocab')\n",
    "TAG_FILE = tf_transform_output.vocabulary_file_by_name('tags')\n",
    "\n",
    "vocab_df = pd.read_csv(VOCAB_FILE, header=None)\n",
    "tags_df = pd.read_csv(TAG_FILE, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>und</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>die</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>der</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>in</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>zu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128968</th>\n",
       "      <td>aalt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128969</th>\n",
       "      <td>aakeel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128970</th>\n",
       "      <td>aafri</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128971</th>\n",
       "      <td>aaargh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128972</th>\n",
       "      <td>aaaaaaaah</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>128973 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                0\n",
       "0             und\n",
       "1             die\n",
       "2             der\n",
       "3              in\n",
       "4              zu\n",
       "...           ...\n",
       "128968       aalt\n",
       "128969     aakeel\n",
       "128970      aafri\n",
       "128971     aaargh\n",
       "128972  aaaaaaaah\n",
       "\n",
       "[128973 rows x 1 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>usa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>factual</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>united kingdom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>reality</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2591</th>\n",
       "      <td>2010-11 football</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2592</th>\n",
       "      <td>2009-10 football</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2593</th>\n",
       "      <td>2000-01 football</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2594</th>\n",
       "      <td>1999-00 football</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2595</th>\n",
       "      <td>1992-93 football</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2596 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     0\n",
       "0                  usa\n",
       "1                drama\n",
       "2              factual\n",
       "3       united kingdom\n",
       "4              reality\n",
       "...                ...\n",
       "2591  2010-11 football\n",
       "2592  2009-10 football\n",
       "2593  2000-01 football\n",
       "2594  1999-00 football\n",
       "2595  1992-93 football\n",
       "\n",
       "[2596 rows x 1 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(vocab_df)\n",
    "display(tags_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems to make sense, though it is slightly concerning that the some of the words in the vocab are things like \"aaaaaaaaaaah\". Is this really in the dataset or was this due to errors in preprocessing? \n",
    "\n",
    "Also very good that we got the same number of tags as before (see `glove_embedding.ipynb` notebook)! \n",
    "\n",
    "We can check that the transformed results make sense:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dataset = tf.data.TFRecordDataset('gs://ml-sandbox-tagging-tfx-experiments/preprocessing_notebook/train_transformed-00000-of-00001')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ================================== Serialized ================================== \n",
      "\n",
      "tf.Tensor(b\"\\n\\xb7\\x07\\n\\x82\\x06\\n\\x08features\\x12\\xf5\\x05\\x1a\\xf2\\x05\\n\\xef\\x05\\n\\xbb\\x0b\\xfa\\xf9\\x05\\x90\\x01\\x0c\\xfb\\x01\\x14\\xaa\\xc4\\x06\\x19\\xd3\\xa7\\x03\\x05\\x10\\x04\\xab\\x07\\xce\\x02\\x1d\\xc4\\x1d\\x0b\\xf8\\xcd\\x05\\xe2\\x9d\\x04\\xe9\\x06\\x03\\x02\\xcc\\x03H\\x16\\x0e\\x06\\x12\\x9f+\\xad\\x1e\\xf1%\\x0c\\x16'\\x1a\\xf6W\\x95\\x03\\xf9\\t\\x00\\xfe\\xaf\\x01#4\\x06\\xc2\\x83\\x01\\x85\\x08\\xcc'\\xb5\\x03\\xd9\\x18\\xdf2\\xf9\\x1c\\xd2\\x05\\x81\\x1e\\x00\\xae\\x02\\x86\\x0b\\x94\\x04,\\xd0\\xd5\\x02g\\xf5\\x06\\xcd\\xef\\x07\\xcd\\xef\\x07\\xcd\\xef\\x07\\xcd\\xef\\x07\\xcd\\xef\\x07\\xcd\\xef\\x07\\xcd\\xef\\x07\\xcd\\xef\\x07\\xcd\\xef\\x07\\xcd\\xef\\x07\\xcd\\xef\\x07\\xcd\\xef\\x07\\xcd\\xef\\x07\\xcd\\xef\\x07\\xcd\\xef\\x07\\xcd\\xef\\x07\\xcd\\xef\\x07\\xcd\\xef\\x07\\xcd\\xef\\x07\\xcd\\xef\\x07\\xcd\\xef\\x07\\xcd\\xef\\x07\\xcd\\xef\\x07\\xcd\\xef\\x07\\xcd\\xef\\x07\\xcd\\xef\\x07\\xcd\\xef\\x07\\xcd\\xef\\x07\\xcd\\xef\\x07\\xcd\\xef\\x07\\xcd\\xef\\x07\\xcd\\xef\\x07\\xcd\\xef\\x07\\xcd\\xef\\x07\\xcd\\xef\\x07\\xcd\\xef\\x07\\xcd\\xef\\x07\\xcd\\xef\\x07\\xcd\\xef\\x07\\xcd\\xef\\x07\\xcd\\xef\\x07\\xcd\\xef\\x07\\xcd\\xef\\x07\\xcd\\xef\\x07\\xcd\\xef\\x07\\xcd\\xef\\x07\\xcd\\xef\\x07\\xcd\\xef\\x07\\xcd\\xef\\x07\\xcd\\xef\\x07\\xcd\\xef\\x07\\xcd\\xef\\x07\\xcd\\xef\\x07\\xcd\\xef\\x07\\xcd\\xef\\x07\\xcd\\xef\\x07\\xcd\\xef\\x07\\xcd\\xef\\x07\\xcd\\xef\\x07\\xcd\\xef\\x07\\xcd\\xef\\x07\\xcd\\xef\\x07\\xcd\\xef\\x07\\xcd\\xef\\x07\\xcd\\xef\\x07\\xcd\\xef\\x07\\xcd\\xef\\x07\\xcd\\xef\\x07\\xcd\\xef\\x07\\xcd\\xef\\x07\\xcd\\xef\\x07\\xcd\\xef\\x07\\xcd\\xef\\x07\\xcd\\xef\\x07\\xcd\\xef\\x07\\xcd\\xef\\x07\\xcd\\xef\\x07\\xcd\\xef\\x07\\xcd\\xef\\x07\\xcd\\xef\\x07\\xcd\\xef\\x07\\xcd\\xef\\x07\\xcd\\xef\\x07\\xcd\\xef\\x07\\xcd\\xef\\x07\\xcd\\xef\\x07\\xcd\\xef\\x07\\xcd\\xef\\x07\\xcd\\xef\\x07\\xcd\\xef\\x07\\xcd\\xef\\x07\\xcd\\xef\\x07\\xcd\\xef\\x07\\xcd\\xef\\x07\\xcd\\xef\\x07\\xcd\\xef\\x07\\xcd\\xef\\x07\\xcd\\xef\\x07\\xcd\\xef\\x07\\xcd\\xef\\x07\\xcd\\xef\\x07\\xcd\\xef\\x07\\xcd\\xef\\x07\\xcd\\xef\\x07\\xcd\\xef\\x07\\xcd\\xef\\x07\\xcd\\xef\\x07\\xcd\\xef\\x07\\xcd\\xef\\x07\\xcd\\xef\\x07\\xcd\\xef\\x07\\xcd\\xef\\x07\\xcd\\xef\\x07\\xcd\\xef\\x07\\xcd\\xef\\x07\\xcd\\xef\\x07\\xcd\\xef\\x07\\xcd\\xef\\x07\\xcd\\xef\\x07\\xcd\\xef\\x07\\xcd\\xef\\x07\\xcd\\xef\\x07\\xcd\\xef\\x07\\xcd\\xef\\x07\\xcd\\xef\\x07\\xcd\\xef\\x07\\xcd\\xef\\x07\\xcd\\xef\\x07\\xcd\\xef\\x07\\xcd\\xef\\x07\\xcd\\xef\\x07\\xcd\\xef\\x07\\xcd\\xef\\x07\\xcd\\xef\\x07\\xcd\\xef\\x07\\xcd\\xef\\x07\\xcd\\xef\\x07\\xcd\\xef\\x07\\xcd\\xef\\x07\\xcd\\xef\\x07\\xcd\\xef\\x07\\xcd\\xef\\x07\\xcd\\xef\\x07\\xcd\\xef\\x07\\xcd\\xef\\x07\\xcd\\xef\\x07\\xcd\\xef\\x07\\xcd\\xef\\x07\\xcd\\xef\\x07\\xcd\\xef\\x07\\xcd\\xef\\x07\\xcd\\xef\\x07\\xcd\\xef\\x07\\xcd\\xef\\x07\\xcd\\xef\\x07\\xcd\\xef\\x07\\xcd\\xef\\x07\\xcd\\xef\\x07\\xcd\\xef\\x07\\xcd\\xef\\x07\\xcd\\xef\\x07\\xcd\\xef\\x07\\xcd\\xef\\x07\\xcd\\xef\\x07\\xcd\\xef\\x07\\xcd\\xef\\x07\\xcd\\xef\\x07\\xcd\\xef\\x07\\xcd\\xef\\x07\\xcd\\xef\\x07\\xcd\\xef\\x07\\xcd\\xef\\x07\\xcd\\xef\\x07\\xcd\\xef\\x07\\xcd\\xef\\x07\\xcd\\xef\\x07\\xcd\\xef\\x07\\xcd\\xef\\x07\\xcd\\xef\\x07\\xcd\\xef\\x07\\xcd\\xef\\x07\\xcd\\xef\\x07\\xcd\\xef\\x07\\xcd\\xef\\x07\\xcd\\xef\\x07\\xcd\\xef\\x07\\xcd\\xef\\x07\\xcd\\xef\\x07\\xcd\\xef\\x07\\xcd\\xef\\x07\\xcd\\xef\\x07\\xcd\\xef\\x07\\xcd\\xef\\x07\\xcd\\xef\\x07\\xcd\\xef\\x07\\xcd\\xef\\x07\\xcd\\xef\\x07\\xcd\\xef\\x07\\xcd\\xef\\x07\\xcd\\xef\\x07\\xcd\\xef\\x07\\xcd\\xef\\x07\\xcd\\xef\\x07\\xcd\\xef\\x07\\xcd\\xef\\x07\\xcd\\xef\\x07\\xcd\\xef\\x07\\xcd\\xef\\x07\\xcd\\xef\\x07\\xcd\\xef\\x07\\xcd\\xef\\x07\\xcd\\xef\\x07\\xcd\\xef\\x07\\xcd\\xef\\x07\\xcd\\xef\\x07\\xcd\\xef\\x07\\n\\xaf\\x01\\n\\x0eseries_ep_tags\\x12\\x9c\\x01\\n\\x99\\x01\\n\\x0c20th century\\n\\x08thriller\\n\\x0fthe middle east\\n\\x07mystery\\n\\x08gulf war\\n\\x03war\\n\\x051990s\\n\\x11ethics & morality\\n\\x12military personnel\\n\\x06action\\n\\x05drama\\n\\x08military\\n\\x0fofficer-colonel\", shape=(), dtype=string)\n",
      "\n",
      " =================================== Decoded ==================================== \n",
      "\n",
      "{'series_ep_tags': <tensorflow.python.framework.sparse_tensor.SparseTensor object at 0x7f912a8c6b90>, 'features': <tf.Tensor: shape=(277,), dtype=int64, numpy=\n",
      "array([    10,   1467,  97530,    144,     12,    251,     20, 107050,\n",
      "           25,  54227,      5,     16,      4,    939,    334,     29,\n",
      "         3780,     11,  91896,  69346,    873,      3,      2,    460,\n",
      "           72,     22,     14,      6,     18,   5535,   3885,   4849,\n",
      "           12,     22,     39,     26,  11254,    405,   1273,      0,\n",
      "        22526,     35,     52,      6,  16834,   1029,   5068,    437,\n",
      "         3161,   6495,   3705,    722,   3841,      0,    302,   1414,\n",
      "          532,     44,  43728,    103,    885, 128973, 128973, 128973,\n",
      "       128973, 128973, 128973, 128973, 128973, 128973, 128973, 128973,\n",
      "       128973, 128973, 128973, 128973, 128973, 128973, 128973, 128973,\n",
      "       128973, 128973, 128973, 128973, 128973, 128973, 128973, 128973,\n",
      "       128973, 128973, 128973, 128973, 128973, 128973, 128973, 128973,\n",
      "       128973, 128973, 128973, 128973, 128973, 128973, 128973, 128973,\n",
      "       128973, 128973, 128973, 128973, 128973, 128973, 128973, 128973,\n",
      "       128973, 128973, 128973, 128973, 128973, 128973, 128973, 128973,\n",
      "       128973, 128973, 128973, 128973, 128973, 128973, 128973, 128973,\n",
      "       128973, 128973, 128973, 128973, 128973, 128973, 128973, 128973,\n",
      "       128973, 128973, 128973, 128973, 128973, 128973, 128973, 128973,\n",
      "       128973, 128973, 128973, 128973, 128973, 128973, 128973, 128973,\n",
      "       128973, 128973, 128973, 128973, 128973, 128973, 128973, 128973,\n",
      "       128973, 128973, 128973, 128973, 128973, 128973, 128973, 128973,\n",
      "       128973, 128973, 128973, 128973, 128973, 128973, 128973, 128973,\n",
      "       128973, 128973, 128973, 128973, 128973, 128973, 128973, 128973,\n",
      "       128973, 128973, 128973, 128973, 128973, 128973, 128973, 128973,\n",
      "       128973, 128973, 128973, 128973, 128973, 128973, 128973, 128973,\n",
      "       128973, 128973, 128973, 128973, 128973, 128973, 128973, 128973,\n",
      "       128973, 128973, 128973, 128973, 128973, 128973, 128973, 128973,\n",
      "       128973, 128973, 128973, 128973, 128973, 128973, 128973, 128973,\n",
      "       128973, 128973, 128973, 128973, 128973, 128973, 128973, 128973,\n",
      "       128973, 128973, 128973, 128973, 128973, 128973, 128973, 128973,\n",
      "       128973, 128973, 128973, 128973, 128973, 128973, 128973, 128973,\n",
      "       128973, 128973, 128973, 128973, 128973, 128973, 128973, 128973,\n",
      "       128973, 128973, 128973, 128973, 128973, 128973, 128973, 128973,\n",
      "       128973, 128973, 128973, 128973, 128973, 128973, 128973, 128973,\n",
      "       128973, 128973, 128973, 128973, 128973])>}\n"
     ]
    }
   ],
   "source": [
    "for raw_record in raw_dataset.take(1):\n",
    "    print('\\n {:=^80} \\n'.format(\" Serialized \"))\n",
    "    print(raw_record)\n",
    "    print('\\n {:=^80} \\n'.format(\" Decoded \"))\n",
    "    print(tf.io.parse_single_example(raw_record, tf_transform_output.transformed_feature_spec()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that transform will write out the data as a TFRecord of TFExample protos. We will need to parse these in order to feed them into the model. Luckily there is a helper function for this: `tf.data.experimental.make_batched_features_dataset`. We will probably end up making use of this later, but for now let's continue exploring some of these examples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>ein</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1467</th>\n",
       "      <td>amerikanischer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97530</th>\n",
       "      <td>militärbeamter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>untersucht</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>den</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251</th>\n",
       "      <td>tod</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>einer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107050</th>\n",
       "      <td>hubschrauberpilotin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>im</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54227</th>\n",
       "      <td>golfkrieg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>von</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>um</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>zu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>939</th>\n",
       "      <td>entscheiden</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>334</th>\n",
       "      <td>ob</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>ihre</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3780</th>\n",
       "      <td>handlungen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>eine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91896</th>\n",
       "      <td>posthume</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69346</th>\n",
       "      <td>ehrenmedaille</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          0\n",
       "10                      ein\n",
       "1467         amerikanischer\n",
       "97530        militärbeamter\n",
       "144              untersucht\n",
       "12                      den\n",
       "251                     tod\n",
       "20                    einer\n",
       "107050  hubschrauberpilotin\n",
       "25                       im\n",
       "54227             golfkrieg\n",
       "5                       von\n",
       "16                       um\n",
       "4                        zu\n",
       "939             entscheiden\n",
       "334                      ob\n",
       "29                     ihre\n",
       "3780             handlungen\n",
       "11                     eine\n",
       "91896              posthume\n",
       "69346         ehrenmedaille"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_example = tf.io.parse_single_example(raw_record, tf_transform_output.transformed_feature_spec())\n",
    "\n",
    "vocab_df.iloc[first_example['features'].numpy()[:20]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like things are working as intended! We are able to decode the first example (compare to the example above)\n",
    "\n",
    "Now let's look at the tags:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[b'20th century' b'thriller' b'the middle east' b'mystery' b'gulf war'\n",
      " b'war' b'1990s' b'ethics & morality' b'military personnel' b'action'\n",
      " b'drama' b'military' b'officer-colonel']\n"
     ]
    }
   ],
   "source": [
    "print(tf.sparse.to_dense(first_example['series_ep_tags']).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, this looks right! We are able to retrieve all of the desired tags. \n",
    "\n",
    "When we actually train the model, we obviously can't use strings like this. Luckily we can create a [`StaticVocabularyTable`](https://www.tensorflow.org/api_docs/python/tf/lookup/StaticVocabularyTable) using a text file initializer based on the vocabulary we just generated in order to go from strings to Id. We can then easily turn those sparse tensors into a multi-one-hot encoded vector in order to actually use this information as labels in the model. See the `Trainer.ipynb` notebook for how this is done."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **B.4 Vocabulary Coverage** \n",
    "\n",
    "Things seem to look good. The last major thing to check is our coverage of the vocabulary. These steps will need to be replicated when we initialize the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "storage_client = storage.Client()\n",
    "\n",
    "bucket = storage_client.bucket('ml-sandbox-101-tagging')\n",
    "blob = bucket.blob('data/processed/training_data/glove_data/glove_embedding_index.pkl')\n",
    "pickle_in = blob.download_as_string()\n",
    "file = pickle.loads(pickle_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(vocab_df) + 1\n",
    "embedding_matrix = np.zeros((vocab_size, 300))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_not_found = []\n",
    "for i, word in enumerate(vocab_df.values):\n",
    "    embedding_vector = file.get(word[0])\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    else:\n",
    "        words_not_found.append(word[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70% of vocabulary is covered by pretrained model\n"
     ]
    }
   ],
   "source": [
    "nonzero_elements = np.count_nonzero(\n",
    "                np.count_nonzero(embedding_matrix, axis=1))\n",
    "overlap = ((nonzero_elements*100)//vocab_size)\n",
    "print(\"{}% of vocabulary is covered by pretrained model\".format(overlap))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['herausforderungen', 'zeichentrickserie', 'eröffnungsausgabe', 'persönlichkeiten', 'eröffnungsfolge', 'verschwenderischen', 'catelynn', 'krankenschwester', 'handlungsstränge', 'vanderpump', 'ausgründungsserie', 'kailyn', 'dokumentarserien', 'zurückzugewinnen', 'swiper', 'stellvertretenden', 'aufrechtzuerhalten', 'jetters', 'immobilienmakler', 'berichterstattung', 'außergewöhnliche', 'zusammenarbeitet', 'auseinandersetzen', 'dokumentarfilmserie', 'wissenschaftliche', 'alleinerziehende', 'tagesslot', 'upsy', 'unwahrscheinlicher', 'liebesinteresse', 'tombliboos', 'igglepiggle', 'geburtstagsfeier', 'jareau', 'nutbrown', 'geschlechtsumwandlungen', 'detektivdrama', 'allgemeinchirurgin', 'polizeikommissar', 'kardiothorakchirurgen', 'unterschiedlichen', 'schnallende', 'raynas', 'kinderanimation', 'jerseylicious', 'diskussionsteilnehmer', 'kardashianern', 'eliminierungsherausforderung', 'zusammenschließen', 'wonnacott']\n",
      "38651\n"
     ]
    }
   ],
   "source": [
    "print(words_not_found[:50])\n",
    "print(len(words_not_found))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good news is that the coverage of the vocabulary is actually a bit higher than before! This is quite positive, though it is possible that this increase in coverage is merely due to the presence of stopwords. \n",
    "\n",
    "More concerning is the nature of the words in the vocabulary not covered by the pretrained embeddings. Above we've printed the first 50 of these words. While it is to be expected that some of these words are not captured (e.g. **jerseylicious, kardashianern**), some of the other non-found words are quite concerning: \n",
    " + Fairly normal words such as *krakenschwester* (nurse) and *persönlichkeiten* (personalities)\n",
    " + Compound words, which are a major feature of the german language: *eröffnungsausgabe* (opening edition), *zurückgewinnen* (to win back). \n",
    " + Words that intuitively would seem quite important for predicting tags: *dokumentarserien*, *dokumentarfilmserie*, *detektivdrama*, *kinderanimation*. These types of words not being captured by the vocabulary might severely limit the ability of the model to predict appropriate tags. \n",
    " \n",
    "One additional thing to note is that it seems like it might be necessary to do stemming in particular cases. Though *persönlichkeiten* is not covered, *persönlichkeit* is: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-3.932790e-01, -1.113070e-01,  2.045410e-01, -1.262810e-01,\n",
       "        2.054620e-01,  9.071800e-02,  2.108400e-01, -6.925700e-02,\n",
       "       -2.625900e-01,  8.015400e-02, -4.734000e-01, -3.358700e-01,\n",
       "       -1.801580e-01, -1.412570e-01,  5.990200e-02, -1.859500e-02,\n",
       "       -4.520390e-01, -6.847640e-01,  4.992600e-02, -6.165300e-02,\n",
       "        4.984950e-01,  2.509250e-01, -1.757640e-01,  5.247500e-02,\n",
       "       -2.351000e-02,  1.460200e-01, -7.447370e-01,  2.156845e+00,\n",
       "       -3.656490e-01, -1.206100e-02, -3.096100e-01, -2.968330e-01,\n",
       "        1.087440e-01, -1.663210e-01,  6.946200e-02, -8.180000e-03,\n",
       "        1.054030e-01, -3.857000e-03, -2.210000e-04,  8.870900e-02,\n",
       "        7.814000e-02, -6.477770e-01,  2.979450e-01, -3.727920e-01,\n",
       "        4.487290e-01,  5.933300e-01,  8.662300e-02,  4.326920e-01,\n",
       "       -5.541000e-02, -3.068400e-02, -1.189760e-01, -1.841990e-01,\n",
       "       -1.053000e-02,  2.140540e-01,  3.582340e-01,  6.302600e-02,\n",
       "        4.267090e-01,  1.622990e-01, -2.603740e-01, -1.479510e-01,\n",
       "       -2.654170e-01,  1.070150e-01, -8.779800e-02,  2.878620e-01,\n",
       "        2.757020e-01, -6.546000e-01, -2.003580e-01, -4.770900e-02,\n",
       "       -3.961900e-01, -1.825840e-01,  1.109400e-01, -5.541120e-01,\n",
       "        3.027990e-01, -3.019550e-01, -8.629700e-02,  1.448550e-01,\n",
       "        3.607350e-01, -7.796200e-02,  2.938770e-01,  2.537300e-01,\n",
       "        3.259590e-01, -3.303960e-01,  4.540760e-01, -1.601800e-01,\n",
       "       -7.241500e-02,  3.819740e-01, -1.013700e-02, -3.127180e-01,\n",
       "       -1.363530e-01,  1.646850e-01,  2.836800e-02, -4.300400e-02,\n",
       "       -4.675100e-02,  4.337010e-01, -1.398010e-01,  5.584700e-02,\n",
       "       -4.679290e-01, -5.252900e-02,  1.086450e-01, -1.149860e-01,\n",
       "        1.810060e-01, -7.521200e-02,  5.699200e-02,  1.267210e-01,\n",
       "       -5.831310e-01,  6.185910e-01, -1.876040e-01,  2.407020e-01,\n",
       "       -1.669900e-01, -3.592540e-01,  6.161100e-02, -2.665960e-01,\n",
       "       -2.185300e-02, -4.351000e-02,  5.866300e-02,  2.343370e-01,\n",
       "       -2.461400e-02,  8.380700e-02,  3.152400e-01,  1.406060e-01,\n",
       "        3.184040e-01,  2.214140e-01,  5.117320e-01, -2.349910e-01,\n",
       "        1.922270e-01,  1.980130e-01, -7.556900e-02, -2.226400e-02,\n",
       "        1.285660e-01,  8.251200e-02, -1.962570e-01,  1.146876e+00,\n",
       "        3.010480e-01, -2.464310e-01, -2.896190e-01, -2.160500e-01,\n",
       "       -6.361410e-01,  2.872750e-01,  2.484900e-02, -7.897300e-01,\n",
       "        4.829290e-01,  4.985200e-02,  2.613600e-01, -5.473350e-01,\n",
       "        4.862320e-01, -5.634300e-02, -2.257850e-01,  1.688300e-01,\n",
       "        1.268800e-01, -2.983850e-01, -1.898670e-01,  1.379420e-01,\n",
       "       -3.471800e-01,  1.184720e-01,  2.470260e-01, -4.799870e-01,\n",
       "        3.596890e-01, -4.020030e-01, -1.625930e-01, -3.267360e-01,\n",
       "       -2.146860e-01, -8.922300e-02,  1.417380e-01, -8.372480e-01,\n",
       "        2.901790e-01,  6.266100e-02,  1.042840e-01,  1.991470e-01,\n",
       "       -2.071730e-01, -2.165440e-01,  3.601100e-01,  1.028960e-01,\n",
       "        2.731220e-01,  2.116800e-01,  1.271540e-01,  1.455060e-01,\n",
       "       -1.176090e-01, -3.255280e-01,  2.234730e-01, -1.908510e-01,\n",
       "       -4.274000e-03,  6.711660e-01,  1.344610e-01,  1.081290e-01,\n",
       "        2.073550e-01, -1.998410e-01,  1.540420e-01,  3.990690e-01,\n",
       "        3.269130e-01,  4.197430e-01,  4.757400e-02,  5.340410e-01,\n",
       "        6.802600e-02, -1.896230e-01, -2.140330e-01,  1.113300e-02,\n",
       "       -3.839290e-01,  2.218140e-01,  5.901200e-02, -5.000660e-01,\n",
       "       -2.215130e-01,  2.936600e-02, -1.917660e-01, -7.263480e-01,\n",
       "       -7.023600e-02,  6.059170e-01, -1.624470e-01,  2.762210e-01,\n",
       "        3.225420e-01,  3.745940e-01,  3.132610e-01, -1.009890e-01,\n",
       "       -8.219010e-01, -1.744160e-01, -6.731000e-02, -8.902000e-03,\n",
       "       -4.986100e-02,  2.742810e-01, -3.119440e-01,  3.064870e-01,\n",
       "       -1.079100e-02, -4.668500e-02, -1.694030e-01,  1.508080e-01,\n",
       "        1.635930e-01, -3.315510e-01, -2.649640e-01,  2.013770e-01,\n",
       "        2.165230e-01, -2.647670e-01,  5.578500e-02, -9.843800e-02,\n",
       "       -4.321800e-01, -4.987650e-01, -2.104280e-01,  1.973090e-01,\n",
       "        1.312630e-01,  5.947880e-01,  2.331050e-01,  1.208130e-01,\n",
       "        3.433200e-02, -1.697370e-01, -1.204400e-01, -1.961000e-02,\n",
       "        2.715810e-01, -2.363000e-01,  3.226550e-01, -6.666430e-01,\n",
       "       -3.952300e-02, -2.162220e-01,  5.768510e-01, -1.665280e-01,\n",
       "        6.224380e-01,  3.524870e-01,  4.095930e-01,  5.149200e-02,\n",
       "        1.406140e-01, -1.012100e-01,  1.271230e-01, -2.931800e-01,\n",
       "       -6.818300e-02, -4.165180e-01, -4.286810e-01,  1.561330e-01,\n",
       "       -1.695560e-01, -1.766090e-01,  1.874080e-01, -3.589300e-02,\n",
       "       -5.475340e-01, -2.920000e-01,  4.411180e-01,  2.803870e-01,\n",
       "       -2.814930e-01, -4.200410e-01, -2.047440e-01,  3.370170e-01,\n",
       "       -3.897410e-01, -4.111200e-01, -2.029060e-01,  3.873070e-01,\n",
       "       -3.018060e-01,  5.967300e-02,  5.366660e-01, -3.401740e-01,\n",
       "       -1.167530e-01,  1.783110e-01, -3.085110e-01,  3.282630e-01,\n",
       "       -1.080690e-01,  2.582910e-01,  4.758500e-01,  4.391810e-01,\n",
       "       -4.403400e-02,  8.466300e-02, -2.252710e-01, -5.569000e-03,\n",
       "        6.748800e-02,  1.419530e-01, -6.066240e-01, -1.853660e-01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file.get('persönlichkeit')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will need to train the model to confirm that performance isn't negatively impacted by this. \n",
    "\n",
    "In general, though, this observation would seem to support the notion that we should explore other methods for embedding text. For future iterations of the model we should look into models in [tf hub](https://tfhub.dev/s?language=de&module-type=text-embedding,text-classification,text-generation,text-language-model,text-question-answering,text-retrieval-question-answering). Using a universal sentence encoder in particular might be quite beneficial; we can basically avoid doing any preprocessing as well as leverage parts of the synopses that are written in other languages (especially english). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Cleanup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf transform_dir"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-2-3-gpu.2-3.m56",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-2-3-gpu.2-3:m56"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
