import tensorflow as tf
import tensorflow_transform as tft
import numpy as np

import transformers as hf


class Tokenizer:
    """This is the base class to tokenize text data.
    Should not be instantiated directly, tokenizer classes specific to each
    type of tokenization should be used instead.
    """

    def __init__(self, max_seq_len, name):
        """
        Argument:
            max_seq_len - int - the fixed length of token lists after padding/
                truncating
            name - string - the tokenizer type
        """
        self.max_seq_len = max_seq_len
        self.name = name

    def from_encoded_tokens_to_string(self, tokens):
        """Returns the string corresponding to the encoded `tokens` using the
        `self.inverse_w_i` inverted word index

        Arguments:
            tokens - numpy array - containing integer token indices

        Returns:
            output - string - corresponding string, retrieved token-by-token
        """
        output = ""
        for index in tokens:
            if index != 0:
                output = output + (self.inverse_w_i[index] + " ")
        return output

    def tokenize(self, texts, fit=False):
        """This method should be overridden by inheriting classes."""
        raise NotImplementedError(
            "You need to use a specific tokenizer (LSTM, Bert...)"
        )

    def _pad_sequences(self, sequences):
        """Pre-pads (or pre-truncates) input sequences to a fixed length of
        `self.max_seq_len`

        Arguments:
            sequences - list - list of list of training encoded tokens

        Returns:
            padded_sequences - list - list of fixed length lists of
                padded/truncated encoded tokens
        """
        padded_sequences = tf.keras.preprocessing.sequence.pad_sequences(
            sequences, maxlen=self.max_seq_len
        )
        return padded_sequences


class BertTokenizer(Tokenizer):
    """This is the Tokenizer class for HuggingFace's pretrained Distilbert
    model's pre-processing
    """

    def __init__(self, max_seq_len, bert_max=None):
        """Sets up the tokenizer and creates the inverse word index (in which
        keys are actual words and values are the associated numeric index
        generated by the encapsulated `self.tokenizer`)

        Arguments:
            max_seq_len - int - the fixed length of token lists after padding/
                truncating
            bert_max - None or int - if int limits the tokens list size when
                encoding before padding and further truncation (needed for
                long texts that Distilbert has a hard time ingesting)
        """
        super(BertTokenizer, self).__init__(max_seq_len, name="bert")
        self.tokenizer = hf.DistilBertTokenizer.from_pretrained(
            "distilbert-base-uncased"
        )
        self.inverse_w_i = self.tokenizer.ids_to_tokens
        self.bert_max = bert_max

    def tokenize(self, texts, fit=False):
        """Tokenizes and pads the list of text examples

        Arguments:
            texts - list - list of free-form text examples
            fit - boolean - placeholder argument here, only necessary to
                overwrite the base class

        Returns:
            padded_sequences - list - list of fixed length lists of
                padded/truncated encoded tokens
        """
        sequences = [
            self.tokenizer.encode(text, max_length=self.bert_max) for text in texts
        ]
        padded_sequences = self._pad_sequences(sequences)
        return padded_sequences
